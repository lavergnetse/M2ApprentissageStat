---
title: "Apprentissage Statistique"
subtitle: "Data Pre Processing - New York City Airbnb"
subsubtitle: "RStudio version 1.2.5001 & R version 3.6.2 (2019-12-12)"
author: "Julien Le Mauff"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    keep_md: yes
    number_sections: true
  pdf_document: 
    keep_tex: yes
    number_sections: true
fontsize: 12pt
always_allow_html: true
header-includes:
- \usepackage{amsmath}
- \newcommand{\E}{\operatorname{E}}
- \newcommand{\Var}{\operatorname{Var}}
- \newcommand{\ind}{\mathbb{I}}
- \newcommand{\tr}{\operatorname{tr}}
- \newcommand{\R}{\mathbb{R}}
- \newcommand{\cvd}{\mbox{$\stackrel{d}{\longrightarrow}\,$}}
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(warning = FALSE, message = FALSE, 
               autodep = TRUE, cache = TRUE, eval = TRUE,
               fig.dim = c(8,3.5), fig.align = 'center')
options(tinytex.verbose = TRUE)
```


```{r Libraries, include=FALSE}
library(dplyr)
library(tidyr)
library(lubridate)
library(stringr)
library(gridExtra)
library(readr)
library(e1071)
library(kernlab)
library(lattice)
library(ggplot2)
library(ggthemes)
```

The aim of this document is to present an overview of the basis concepts of the data pre-processing. We mean by the term pre-processing the step of a Machine Learning project in which data gets transformed so that the machine can easily analyze it, especially when performing algorithms. Indeed raw data face a large number of issues which the best known are missing data, inconsistent and dupplicated values, outliers, categorical variable and feature scaling. All these points must be taken into account in order to carry out a project, and this first task is a major step in a ML project. Sophisticated algorithms will not compensate for bad data.

So without wasting time, let's take a closer look at the different steps of the data Pre Processing!

# Data formatting

Before going into the details of data cleaning, you have make sure that your data are in the right format, which means managing with these different tasks:

## Variable selection 

A lot of dataset are available online in csv formats. The following work is based on the open data from *[inside Airbnb] (http://insideairbnb.com/get-the-data.html)*. Our objective will be to set up a model allowing to forecast the price of the housing available on the platform. In this setting we will focus on the housing available in New York city. 

```{r import raw data}
# Import the downloaded data
files <- read.csv("https://raw.githubusercontent.com/lavergnetse/Data/master/listings.csv", sep = ",")

# Let's have a first overview of our dataset
dplyr::glimpse(files)
```

The `glimpse` function allows to see a first global view of our raw data. And as we can see, our dataset contains 36 variables, with some of them which may not be relevant to our project. We thus have to do a first sort and keep only the relevant ones, that is to say all the variables whose value can have an impact on the housing price. This selection task can be realized using empirical results, literature review, etc. 
In our example, the predictors we need are those that give information about the housing (location, attributes, rating), the host attributes and the rental rules.

```{r select predictors}
# Select the columns we want to keep from the initial listings csv files
listings_keep <- c("id", "price", "host_is_superhost",
                   "neighbourhood_group_cleansed", "neighbourhood_cleansed",
                   "latitude","longitude", "property_type", "accommodates",
                   "bedrooms", "beds", "bed_type", "cleaning_fee",
                   "minimum_nights", "availability_365",
                   "review_scores_rating","cancellation_policy")

listings <-  files[,listings_keep]
```

## Dealing with inconsistent values

As we saw before, some values are meaningless and must be treated in order to get an operational database. First of all, we need to focus on the id of the housing. Indeed when we deal with large database, we often face duplicated values that we have to drop.  

- Eliminate all duplicates listings

```{r 1st inconsistent values}
listings <- listings[!duplicated(listings$id),]
```

- Eliminate observations with missing id

```{r}
listings <- listings[complete.cases(listings$id),]
```

Then another common thing to do is to look at the class of each variables, focusing particularly on the numerical ones, and also at the summary of your data. This work will help you to have a first look on your data, and see the points that you might find surprising.   

```{r check data structure}
# Check the statistical summary of each variables
library(Hmisc)
Hmisc::describe(listings)
```

We can thus see that:

- *price* and *cleaning_fee* are stored as factor instead of numeric

```{r 2nd inconsistent values}
# Eliminate the $ sign on price
listings$price = as.numeric(gsub("[\\$,]", "", listings$price))

# Eliminate the $ sign on cleaning fee
listings$cleaning_fee = as.numeric(gsub("[\\$,]", "", listings$cleaning_fee))
```

- *host_is_superhost* presents 11 missing values that are not indexed as NA's  

```{r }
# Replace the empty cells by NA's
listings$host_is_superhost[listings$host_is_superhost==""] <- NA
```

- there are observations where *price* or *availability_365* are equal to 0

```{r }
# For the sake of simplicity, we delete the observations where price is null
listings <- listings %>% filter(price > 0) 

# Almost 40% of the observations (20687) with availability_365 = 0 
listings %>% count (availability_365 == 0)
```

Such a high number let us think that it is not due to bad data. Conversely, one would therefore think that the listing were not available at the moment of data collection. In consequence those hosts had their listing availability set to 0 or «Not available». 

Finally we can suspect the presence of inconsistencies in our data, such as observations where the accommodate capacity is lower than the number of beds, so we check for that and remove those observations if necessary:

```{r }
# For the sake of simplicity, we delete the observations where price is null
listings <- listings %>% filter(accommodates >= beds) 
```

Once we have complete this first step, we can export the database in our repository. This step is not obligatory, but it allows to keep track of our data before cleaning it.

```{r export new data, eval=FALSE}
# Export the listing compiled database
write.csv(listings, "/Users/.../data_compiled.csv", row.names = FALSE)
```

# Data cleaning

We import our new database: 

```{r factor into character, include=FALSE}
mydata <- read.csv("https://raw.githubusercontent.com/lavergnetse/Data/master/data_compiled.csv", sep = ",", stringsAsFactors = T)
```

## Dealing with missing values

Most of the time, you will see that your database include missing data, and dealing with this issue is mandatory in all data pre processing situation. In the following, you will see some basic methods that allow to deal with this issue.

The code below allows you to see the number of NA for each variable. 

```{r count NA}
# NA count 
na_count <- as.data.frame(colSums(mydata%>%is.na(.)))
na_count
```

- **Bedrooms**

```{r NA correction Bedrooms, echo=FALSE}
ggplot(mydata) +
  geom_histogram(aes(x = bedrooms), colour="white", fill = "skyblue") + 
  ggtitle("Distribution bedrooms") +
  theme_economist()
```

Given the distribution, we could replace the 112 missing values of bedrooms (the number of bedrooms) by the median grouped by property type, number of accommodates and bed types.

We would use the median because we want an integer and not a float (numbers with commas). Furthermore, the median works great against skewed distribution and outliers.

```{r}
mydata <- mydata %>%
  group_by(
    property_type,
    accommodates) %>%
  mutate(bedrooms=ifelse(is.na(bedrooms),
                         median(bedrooms,na.rm = T),bedrooms)) %>%
  ungroup()
```

- **Cleaning fee**

A cleaning fee is a one-time fee charged by hosts to cover the cost of cleaning their rental when guests depart. Not all hosts charge this fee. Some incorporate it into their nightly rate. Therefore it is safe to assume that when there is no value for this
variable it simply means that the host didn't charge

```{r NA correction Cleaning fee}
mydata[is.na(mydata$cleaning_fee), "cleaning_fee"] <- 0
```

- **Host is superhost**

There is only 11 missing values out of more than 500000.
Therefore we will just replace them by the most common value (f)

```{r NA correction Host is superhost}
mydata[is.na(mydata$host_is_superhost),"host_is_superhost"] <- "f"
```

- **Review scores rating**

```{r NA correction Review scores rating, echo=FALSE}
ggplot(mydata) +
  geom_histogram(aes(x = review_scores_rating), colour="white", fill = "skyblue") + 
  ggtitle("Distribution Review scores rating") +
  theme_economist()
```

Given that the distribution is skewed on the right, we will use the median to approximate the missing values grouped by neighbors, property types, type of beds and if the host is a superhost.

```{r}
mydata <- mydata %>%
  group_by(neighbourhood_cleansed,
           bed_type,
           property_type,
           host_is_superhost) %>%
  mutate(review_scores_rating=ifelse(is.na(review_scores_rating), 
                                     median(review_scores_rating,na.rm=T),
                                     review_scores_rating)) %>%
  ungroup()
```


There is still 216 missing values so we could just replace them with the overall score median 

```{r}
mydata[is.na(mydata$review_scores_rating), "review_scores_rating"] <-
  median(mydata$review_scores_rating, na.rm = T)
```

We do a final check:

```{r}
sum(is.na(mydata))
```

As you can see, we are trying to avoid simply eliminating rows with missing data. Indeed with large number of observations and small proportions of missing data, remove them will not have a large impact on the results. However doing so with a higher proportion of NA's may removing some crucial information. That's why you should prefer the alternatives showed here : mean or median imputation using the `group_by` function. With more complex data, others imputation methods such as `Knn` or `Missforest` can be used.

## Dealing with categorical variables 

Here are the tree variables that we want to reorganize by creating new subgroups:

```{r categorical variables}
# Property type
levels(mydata$property_type)

# Cancellation policy

ggplot(mydata, aes (x = cancellation_policy)) +
    geom_bar(width = 1, colour = "white", fill="skyblue", show.legend = F,
             aes(y = ((..count..)/sum(..count..)))) +
    geom_text(aes(y = ((..count..)/sum(..count..)),
                  label = scales::percent((..count..)/sum(..count..))),
              stat = "count", size=3, vjust = -.3) +
    scale_y_continuous(labels = scales::percent) +
    labs(x = "", y = "") + 
    ggtitle("Cancellation Policy") +
    theme_economist()

#Bed_type

ggplot(mydata, aes (x = bed_type)) +
    geom_bar(width = 1, colour = "white", fill="skyblue", show.legend = F,
             aes(y = ((..count..)/sum(..count..)))) +
    geom_text(aes(y = ((..count..)/sum(..count..)),
                  label = scales::percent((..count..)/sum(..count..))),
              stat = "count", size=3, vjust = -.3) +
    scale_y_continuous(labels = scales::percent) +
    labs(x = "", y = "") + 
    ggtitle("Type of bed") +
   theme_economist()
```

\
- Subgroup for **Property type**

We groups all 40 subgroups of housing types into 5 big groups:

```{r}
Appartment <- c("Aparthotel","Serviced apartment", "Loft",
                "Condominium", "Apartment")

House <- c("Barn", "Dome house", "Lighthouse", "Houseboat",
           "Treehouse", "Earth house", "Cottage", "Tiny house",
           "Townhouse", "House", "Cabin","Villa")

Shared_room <- c("Dorm", "Hostel", "Guesthouse", "Timeshare")

Private_room <- c("Farm stay", "Bed and breakfast", "Resort", "Hotel",
                  "Boutique hotel", "Guest suite", "In-law")

Other <- c("Bungalow", "Train", "Bus", "Boat", "Other", "Cave", "Island",
           "Camper/RV", "Yurt", "Castle", "Tent", "Casa particular (Cuba)")

mydata$property_type <- as.character(mydata$property_type)

mydata <-
  mutate(mydata,
         property_type = ifelse(property_type %in% Appartment,
                                "Appartment", property_type),
         property_type = ifelse(property_type %in% House,
                                "House", property_type),
         property_type = ifelse(property_type %in% Shared_room,
                                "SharedRoom", property_type),
         property_type = ifelse(property_type %in% Private_room,
                                "PrivateRoom", property_type),
         property_type = ifelse(property_type %in% Other,
                                "Others", property_type))
```

Now we can plot our new variable:

```{r}
# Property type
ggplot(mydata, aes (x = property_type)) +
    geom_bar(width = 1, colour = "white", fill="skyblue", show.legend = F,
             aes(y = ((..count..)/sum(..count..)))) +
    geom_text(aes(y = ((..count..)/sum(..count..)),
                  label = scales::percent((..count..)/sum(..count..))),
              stat = "count", size=3, vjust = -.3) +
    scale_y_continuous(labels = scales::percent) +
    labs(x = "", y = "") + 
    ggtitle("Property Type") +
  theme_economist()
```

Then we remove the observation associated to the property type "Other" & "SharedRoom" 
since it brings together a set of marginal housing category

```{r}
mydata <- mydata[!mydata$property_type=="Others",]
mydata <- mydata[!mydata$property_type=="SharedRoom",]
```

```{r, echo=FALSE}
# Property type
ggplot(mydata, aes (x = property_type)) +
    geom_bar(width = 1, colour = "white", fill="skyblue", show.legend = F,
             aes(y = ((..count..)/sum(..count..)))) +
    geom_text(aes(y = ((..count..)/sum(..count..)),
                  label = scales::percent((..count..)/sum(..count..))),
              stat = "count", size=3, vjust = -.3) +
    scale_y_continuous(labels = scales::percent) +
    labs(x = "", y = "") + 
    ggtitle("Property Type") +
  theme_economist()
```

\
- Subgroup for **Cancellation policy**

We groups all 4 subgroups of strict types into only group

```{r}
mydata$cancellation_policy <- as.character(mydata$cancellation_policy)

mydata <- mutate(mydata, 
               cancellation_policy = 
                 ifelse(cancellation_policy=="strict_14_with_grace_period",
                        "strict", cancellation_policy),
               cancellation_policy =
                 ifelse(cancellation_policy=="super_strict_30", 
                        "strict", cancellation_policy),
               cancellation_policy =
                 ifelse(cancellation_policy=="super_strict_60", 
                        "strict", cancellation_policy))
```

```{r, echo=FALSE}
ggplot(mydata, aes (x = cancellation_policy)) +
    geom_bar(width = 1, colour = "white", fill="skyblue", show.legend = F,
             aes(y = ((..count..)/sum(..count..)))) +
    geom_text(aes(y = ((..count..)/sum(..count..)),
                  label = scales::percent((..count..)/sum(..count..))),
              stat = "count", size=3, vjust = -.3) +
    scale_y_continuous(labels = scales::percent) +
    labs(x = "", y = "") + 
    ggtitle("Cancellation Policy") +
    theme_economist()
```

\
- Subgroup for **Bed Types**

Since only a handful of 699 observations out of more than 50000 housing
have their bed type value which is different from "Real Bed", we can either spread these
marginal observations into a subgroup "others" or remove this variable (which is finally
meaningless).
In this example we decided to remove bed_type

```{r}
mydata <- mydata[,-which(names(mydata) == "bed_type")]
```

\
- Subgroup for **Host status**

For the reading we can change the "t" to "True" and "f" to "False":

```{r}
mydata <- mutate(mydata, 
               host_is_superhost = ifelse(host_is_superhost=="t", 
                                          "True", "False"))
```

We finally export our cleaned database for the next steps of the project.

```{r export cleaned data, eval = FALSE}
write.csv(mydata, "/Users/lemauffjulien/Desktop/data/clean_data.csv", row.names = FALSE) 
```
